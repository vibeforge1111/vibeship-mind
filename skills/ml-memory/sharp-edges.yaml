# ML Memory Sharp Edges
# Battle scars from building AI memory systems at scale

sharp_edges:
  - id: entity-duplication-explosion
    summary: Same entity appears hundreds of times with different names
    severity: critical
    situation: |
      You ingest user conversations without entity resolution. Months later,
      you realize "John", "John Smith", "my husband", "him", and "Dr. Smith"
      are all the same person with 5 different memory graphs.
    why: |
      Users refer to entities in many ways. Without resolution, each variant
      creates new nodes. Queries return fragmented results. Memory is split
      across entities that should be unified.
    solution: |
      # Implement entity resolution pipeline
      class EntityResolver:
          CONFIDENCE_THRESHOLD = 0.7

          async def resolve_or_create(
              self,
              mention: str,
              context: str,
              user_id: UUID,
          ) -> Entity:
              # 1. Exact match first
              exact = await self.db.find_entity_exact(mention, user_id)
              if exact:
                  return exact

              # 2. Semantic similarity search
              candidates = await self.find_similar_entities(
                  mention, context, user_id
              )

              for candidate in candidates:
                  # 3. LLM-based coreference resolution
                  is_same, confidence = await self.llm_coreference_check(
                      mention, context, candidate
                  )

                  if is_same and confidence > self.CONFIDENCE_THRESHOLD:
                      # 4. Add alias to existing entity
                      await self.add_alias(candidate, mention)
                      return candidate

              # 5. Create new entity only if no match
              return await self.create_entity(mention, user_id)

      # Run periodic entity merge jobs to fix historical issues
    symptoms:
      - "Memory graph has duplicate nodes for same person"
      - "Queries miss relevant memories because entity name differs"
      - "Entity count growing linearly with conversation length"
      - "User says 'I already told you about John' but system doesn't find it"
    detection_pattern: 'store.*entity|create.*entity|new.*Entity'
    version_range: ">=1.0.0"

  - id: zep-graphiti-llm-costs
    summary: LLM calls during every ingestion creates massive API bills
    severity: high
    situation: |
      You use Graphiti/Zep's entity extraction which calls LLM for every
      ingested message. At scale, you're making 100K+ LLM calls per day
      just for entity extraction.
    why: |
      Graphiti uses LLMs for entity extraction, relationship detection, and
      summarization. Each ingested message can trigger 2-5 LLM calls.
      At conversation scale, this becomes prohibitively expensive.
    solution: |
      # Batch extraction to reduce LLM calls
      class BatchEntityExtractor:
          BATCH_SIZE = 10
          MAX_BATCH_DELAY = timedelta(seconds=30)

          def __init__(self):
              self.pending_messages = []
              self.last_flush = datetime.utcnow()

          async def queue_for_extraction(self, message: Message) -> None:
              self.pending_messages.append(message)

              if (
                  len(self.pending_messages) >= self.BATCH_SIZE or
                  datetime.utcnow() - self.last_flush > self.MAX_BATCH_DELAY
              ):
                  await self.flush()

          async def flush(self) -> List[Entity]:
              if not self.pending_messages:
                  return []

              # Single LLM call for entire batch
              combined_text = "\n---\n".join(
                  m.content for m in self.pending_messages
              )

              entities = await self.llm.extract_entities(combined_text)

              # Clear batch
              self.pending_messages = []
              self.last_flush = datetime.utcnow()

              return entities

      # Also consider: local NER models for first pass
      # spacy, GLiNER, or fine-tuned BERT for entity extraction
      # Only use LLM for relationship inference
    symptoms:
      - "API costs growing linearly with message volume"
      - "Ingestion latency high due to LLM calls"
      - "Rate limit errors from embedding/LLM provider"
      - "Entity extraction is slowest part of pipeline"
    detection_pattern: 'graphiti|zep.*add_episode|extract.*entities.*await'
    version_range: ">=1.0.0"

  - id: temporal-staleness-current
    summary: "Current" facts become stale without temporal validity
    severity: high
    situation: |
      You store "user works at Google" as a fact. Six months later,
      user mentions new job. System still retrieves old job as current
      because it has higher salience.
    why: |
      Facts have temporal validity. "Current job" changes. "Lives in"
      changes. Without valid_until handling, stale facts accumulate
      and pollute retrieval.
    solution: |
      # Always store temporal validity for mutable facts
      @dataclass
      class MemoryFact:
          entity_id: UUID
          predicate: str  # "works_at", "lives_in"
          value: str
          valid_from: datetime
          valid_until: Optional[datetime]  # None = still valid

          # Mutable predicates that need temporal handling
          MUTABLE_PREDICATES = {
              "works_at", "lives_in", "married_to", "age",
              "role", "company", "location", "status",
          }

      class TemporalFactStore:
          async def add_fact(
              self,
              entity_id: UUID,
              predicate: str,
              value: str,
          ) -> MemoryFact:
              # If mutable predicate, close previous fact
              if predicate in MemoryFact.MUTABLE_PREDICATES:
                  await self.db.execute(
                      """
                      UPDATE memory_facts
                      SET valid_until = NOW()
                      WHERE entity_id = $1
                        AND predicate = $2
                        AND valid_until IS NULL
                      """,
                      entity_id, predicate
                  )

              # Insert new fact
              return await self.db.insert_fact(
                  entity_id=entity_id,
                  predicate=predicate,
                  value=value,
                  valid_from=datetime.utcnow(),
                  valid_until=None,
              )

          async def get_current_facts(self, entity_id: UUID) -> List[MemoryFact]:
              # Only return currently valid facts
              return await self.db.query(
                  """
                  SELECT * FROM memory_facts
                  WHERE entity_id = $1 AND valid_until IS NULL
                  """
              )
    symptoms:
      - "Old facts returned instead of current ones"
      - "User corrects 'I don't work there anymore'"
      - "Contradictory facts about same entity"
      - "Memory shows outdated information"
    detection_pattern: 'store.*fact|insert.*fact(?!.*valid_until)'
    version_range: ">=1.0.0"

  - id: consolidation-conflicts
    summary: Concurrent consolidation corrupts memory state
    severity: high
    situation: |
      You run consolidation jobs on a schedule. Two jobs overlap because
      the first one took longer than expected. Memories get promoted twice
      or corrupted.
    why: |
      Consolidation reads, processes, and writes memory state. Without
      locking, concurrent jobs read same state, make conflicting decisions,
      and corrupt data.
    solution: |
      # Use distributed locking for consolidation
      from redis.lock import Lock

      class SafeConsolidator:
          LOCK_TIMEOUT = timedelta(minutes=30)

          async def consolidate(self, user_id: UUID) -> ConsolidationResult:
              lock_key = f"consolidation:{user_id}"

              # Acquire distributed lock
              async with self.redis.lock(
                  lock_key,
                  timeout=self.LOCK_TIMEOUT.total_seconds(),
                  blocking=False,  # Don't wait, skip if locked
              ) as lock:
                  if not lock:
                      logger.info(f"Consolidation already running for {user_id}")
                      return ConsolidationResult(skipped=True)

                  # Use optimistic locking for individual memory updates
                  memories = await self.db.get_memories_for_consolidation(
                      user_id, for_update=True  # SELECT FOR UPDATE
                  )

                  for memory in memories:
                      await self._process_memory(memory)

                  return ConsolidationResult(processed=len(memories))

          async def _process_memory(self, memory: Memory) -> None:
              # Include version check in update
              result = await self.db.execute(
                  """
                  UPDATE memories
                  SET temporal_level = $1, version = version + 1
                  WHERE memory_id = $2 AND version = $3
                  """,
                  new_level, memory.memory_id, memory.version
              )

              if result.rowcount == 0:
                  raise ConcurrencyError(f"Memory {memory.memory_id} modified")
    symptoms:
      - "Memories promoted multiple times"
      - "Consolidation logs show overlapping runs"
      - "Inconsistent memory counts after consolidation"
      - "Database deadlocks during consolidation"
    detection_pattern: 'consolidat.*async|schedule.*consolidat'
    version_range: ">=1.0.0"

  - id: salience-never-decreases
    summary: Salience only increases, noise accumulates
    severity: medium
    situation: |
      You implement salience boosting when memories are accessed. Over time,
      all frequently accessed memories have max salience. The ranking becomes
      meaningless.
    why: |
      Without decrease mechanism, salience saturates. Old but once-useful
      memories stay at top. New important memories can't compete. The
      system becomes increasingly noisy.
    solution: |
      # Implement bidirectional salience adjustment
      class BidirectionalSalience:
          BOOST_RATE = 0.1
          DECAY_RATE = 0.02
          OUTCOME_WEIGHT = 0.15

          async def on_memory_used(
              self,
              memory_id: UUID,
              outcome: Optional[float],  # -1 to 1, None if unknown
          ) -> None:
              memory = await self.db.get_memory(memory_id)

              if outcome is not None:
                  # Outcome-based: boost if good, penalize if bad
                  adjustment = outcome * self.OUTCOME_WEIGHT
              else:
                  # Access-based: small boost for being useful enough to retrieve
                  adjustment = self.BOOST_RATE

              new_salience = max(0.01, min(1.0, memory.salience + adjustment))
              await self.db.update_salience(memory_id, new_salience)

          async def apply_periodic_decay(self, user_id: UUID) -> None:
              """Run daily to decay all memories slightly."""
              await self.db.execute(
                  """
                  UPDATE memories
                  SET salience = GREATEST(0.01, salience - $1)
                  WHERE user_id = $2
                    AND last_accessed < NOW() - INTERVAL '24 hours'
                  """,
                  self.DECAY_RATE, user_id
              )

          async def penalize_unused(self, user_id: UUID) -> None:
              """Penalize memories that were retrieved but not used."""
              await self.db.execute(
                  """
                  UPDATE memories
                  SET salience = GREATEST(0.01, salience - 0.05)
                  WHERE memory_id IN (
                      SELECT memory_id FROM retrieval_log
                      WHERE retrieved_at > NOW() - INTERVAL '24 hours'
                        AND was_used = FALSE
                  )
                  """
              )
    symptoms:
      - "Most memories at maximum salience"
      - "New memories hard to surface"
      - "Retrieval quality degrading over time"
      - "Old irrelevant memories dominating results"
    detection_pattern: 'salience.*\\+|boost.*salience|increase.*salience'
    version_range: ">=1.0.0"

  - id: forgetting-breaks-references
    summary: Forgotten memories leave dangling references
    severity: medium
    situation: |
      You implement memory forgetting to manage storage. A memory is forgotten,
      but graph edges still point to it. Entity relationships break.
    why: |
      Memories connect to entities, other memories, and decision traces.
      Hard delete without cascade leaves orphaned references and broken
      relationships.
    solution: |
      # Implement soft delete with reference handling
      class SafeForgetting:
          async def forget_memory(
              self,
              memory_id: UUID,
              hard_delete: bool = False,
          ) -> None:
              # 1. Check for active references
              references = await self.db.count_references(memory_id)

              if references > 0 and not hard_delete:
                  # Soft delete: mark as forgotten but keep for references
                  await self.db.execute(
                      """
                      UPDATE memories
                      SET status = 'forgotten',
                          content = '[FORGOTTEN]',  # Clear content
                          embedding = NULL
                      WHERE memory_id = $1
                      """,
                      memory_id
                  )
              else:
                  # Hard delete: must clean up references first
                  async with self.db.transaction():
                      # Remove graph edges
                      await self.graph.delete_memory_edges(memory_id)

                      # Remove from decision traces (or mark as unavailable)
                      await self.db.execute(
                          """
                          UPDATE decision_memory_attribution
                          SET memory_available = FALSE
                          WHERE memory_id = $1
                          """,
                          memory_id
                      )

                      # Now safe to delete
                      await self.db.delete_memory(memory_id)

          async def cleanup_orphaned_references(self) -> int:
              """Periodic job to clean up dangling references."""
              # Find edges pointing to non-existent memories
              orphans = await self.db.query(
                  """
                  SELECT edge_id FROM memory_edges e
                  LEFT JOIN memories m ON e.target_id = m.memory_id
                  WHERE m.memory_id IS NULL
                  """
              )

              for orphan in orphans:
                  await self.graph.delete_edge(orphan.edge_id)

              return len(orphans)
    symptoms:
      - "Graph queries return null for memory properties"
      - "Foreign key violations when deleting memories"
      - "'Memory not found' errors in decision traces"
      - "Orphaned edges accumulating in graph"
    detection_pattern: 'delete.*memory|remove.*memory|forget.*memory'
    version_range: ">=1.0.0"

  - id: embedding-model-mismatch
    summary: Query embeddings from different model than stored embeddings
    severity: high
    situation: |
      You upgrade your embedding model. New queries use the new model,
      but stored embeddings are from the old model. Similarity search
      returns garbage.
    why: |
      Different embedding models produce incompatible vector spaces.
      "text-embedding-3-small" vectors are meaningless when compared to
      "text-embedding-ada-002" vectors. Cosine similarity becomes random.
    solution: |
      # Track embedding model version with each memory
      @dataclass
      class Memory:
          memory_id: UUID
          content: str
          embedding: List[float]
          embedding_model: str  # "text-embedding-3-small"
          embedding_version: str  # "v1.0"

      class EmbeddingMigrator:
          async def migrate_embeddings(
              self,
              target_model: str,
              batch_size: int = 100,
          ) -> MigrationResult:
              """Re-embed all memories with new model."""
              cursor = None
              migrated = 0

              while True:
                  # Get batch of memories with old model
                  memories = await self.db.query(
                      """
                      SELECT memory_id, content FROM memories
                      WHERE embedding_model != $1
                      ORDER BY memory_id
                      LIMIT $2 OFFSET $3
                      """,
                      target_model, batch_size, cursor or 0
                  )

                  if not memories:
                      break

                  # Batch embed
                  contents = [m.content for m in memories]
                  new_embeddings = await self.embedder.embed_batch(
                      contents, model=target_model
                  )

                  # Update
                  for memory, embedding in zip(memories, new_embeddings):
                      await self.db.execute(
                          """
                          UPDATE memories
                          SET embedding = $1, embedding_model = $2
                          WHERE memory_id = $3
                          """,
                          embedding, target_model, memory.memory_id
                      )

                  migrated += len(memories)
                  cursor = (cursor or 0) + batch_size

              return MigrationResult(migrated=migrated)

      # Query must use same model as stored embeddings
      async def search(self, query: str, user_id: UUID) -> List[Memory]:
          # Get user's embedding model
          model = await self.db.get_user_embedding_model(user_id)

          # Embed query with same model
          query_embedding = await self.embedder.embed(query, model=model)

          return await self.vector_store.search(query_embedding)
    symptoms:
      - "Vector search returns irrelevant results"
      - "Similarity scores near zero or random"
      - "Search quality degraded after model update"
      - "Some memories never retrieved"
    detection_pattern: 'embed.*model|embedding_model|text-embedding'
    version_range: ">=1.0.0"

  - id: memory-attribution-missing
    summary: Can't learn from outcomes without tracking which memories were used
    severity: medium
    situation: |
      You want to implement outcome-based learning but realize you never
      tracked which memories influenced each decision. You have outcomes
      but no way to credit or blame memories.
    why: |
      Outcome learning requires attribution: which memories were retrieved,
      which were actually used, and what the outcome was. Without this trace,
      you can't close the learning loop.
    solution: |
      # Implement decision tracing from day one
      @dataclass
      class DecisionTrace:
          trace_id: UUID
          user_id: UUID
          decision_context: str  # What was the user asking
          timestamp: datetime

          # Memory attribution
          memories_retrieved: List[UUID]  # What was found
          memories_used: List[UUID]  # What was actually in the response
          memory_influence: Dict[UUID, float]  # How much each influenced

          # Outcome (filled later)
          outcome_quality: Optional[float]  # -1 to 1
          outcome_source: Optional[str]  # "explicit", "implicit", "inferred"

      class DecisionTracer:
          async def start_trace(
              self,
              user_id: UUID,
              context: str,
          ) -> DecisionTrace:
              trace = DecisionTrace(
                  trace_id=uuid4(),
                  user_id=user_id,
                  decision_context=context,
                  timestamp=datetime.utcnow(),
                  memories_retrieved=[],
                  memories_used=[],
                  memory_influence={},
              )
              await self.db.insert_trace(trace)
              return trace

          async def record_retrieval(
              self,
              trace: DecisionTrace,
              memory_ids: List[UUID],
          ) -> None:
              trace.memories_retrieved = memory_ids
              await self.db.update_trace(trace)

          async def record_usage(
              self,
              trace: DecisionTrace,
              memory_ids: List[UUID],
              influence_scores: Dict[UUID, float],
          ) -> None:
              trace.memories_used = memory_ids
              trace.memory_influence = influence_scores
              await self.db.update_trace(trace)

          async def record_outcome(
              self,
              trace_id: UUID,
              quality: float,
              source: str,
          ) -> None:
              await self.db.execute(
                  """
                  UPDATE decision_traces
                  SET outcome_quality = $1, outcome_source = $2
                  WHERE trace_id = $3
                  """,
                  quality, source, trace_id
              )

              # Trigger salience learning
              trace = await self.db.get_trace(trace_id)
              await self.salience_learner.update_from_outcome(trace)
    symptoms:
      - "Can't implement outcome-based learning"
      - "No data on which memories were useful"
      - "Salience remains static"
      - "Can't debug why bad memories were retrieved"
    detection_pattern: 'retrieve.*memor|search.*memor(?!.*trace|.*log)'
    version_range: ">=1.0.0"

  # Mind v5 Specific Sharp Edges
  - id: mind-v5-retrieval-no-decision-trace
    summary: Memory retrieval without creating decision trace
    severity: critical
    situation: |
      You retrieve memories for a decision but don't create a DecisionTrace.
      Later, you can't attribute outcomes back to memories that influenced them.
    why: |
      Mind v5's core value proposition is learning from outcomes. Without traces,
      salience learning is impossible. The system never gets better.
    solution: |
      # Always wrap retrieval in decision trace context
      async def retrieve_for_decision(
          user_id: UUID,
          query: str,
          context: DecisionContext,
      ) -> Tuple[List[Memory], DecisionTrace]:
          # Start trace BEFORE retrieval
          trace = await self.tracer.start_trace(user_id, query)

          try:
              memories = await self.retriever.retrieve(user_id, query)

              # Record what was retrieved
              await self.tracer.record_retrieval(
                  trace,
                  memory_ids=[m.memory_id for m in memories],
                  relevance_scores={m.memory_id: m.score for m in memories},
              )

              return memories, trace
          except Exception as e:
              await self.tracer.record_error(trace, e)
              raise

      # After decision is made, record usage
      async def finalize_decision(
          trace: DecisionTrace,
          used_memory_ids: List[UUID],
          influence_scores: Dict[UUID, float],
      ) -> None:
          await self.tracer.record_usage(trace, used_memory_ids, influence_scores)
    symptoms:
      - "Salience scores remain static over time"
      - "No way to know which memories helped decisions"
      - "Can't improve retrieval quality from feedback"
    detection_pattern: 'retrieve.*memor(?!.*trace|.*DecisionTrace)'
    version_range: ">=5.0.0"

  - id: mind-v5-identity-memory-forgetting
    summary: Applying forgetting to identity-level memories
    severity: high
    situation: |
      Your decay job runs across all temporal levels. It forgets identity-level
      memories like "user is vegetarian" or "user's mother is named Sarah".
    why: |
      Identity memories are the most valuable in Mind v5. They're promoted
      after extensive evidence (20+ confirmations). Forgetting them loses
      hard-won knowledge that took months to accumulate.
    solution: |
      # Exempt identity level from decay
      async def apply_decay(user_id: UUID) -> DecayResult:
          result = await self.db.execute(
              """
              UPDATE memories
              SET base_salience = GREATEST(0.05, base_salience * 0.95)
              WHERE user_id = $1
                AND temporal_level != 4  -- NEVER decay identity (level 4)
                AND last_accessed < NOW() - INTERVAL '24 hours'
              RETURNING memory_id
              """,
              user_id
          )
          return DecayResult(decayed_count=len(result))

      # Identity memories have separate maintenance
      async def consolidate_identity(user_id: UUID) -> None:
          # Only merge duplicates, never delete
          await self._merge_duplicate_identities(user_id)
          # Update confidence based on continued evidence
          await self._update_identity_confidence(user_id)
    symptoms:
      - "User says 'I already told you I'm vegetarian' repeatedly"
      - "Core preferences lost over time"
      - "Identity memories need re-learning"
    detection_pattern: 'decay.*memor(?!.*temporal_level|.*identity|.*level.*4)'
    version_range: ">=5.0.0"

  - id: mind-v5-outcome-not-bounded
    summary: Outcome adjustments can overflow salience bounds
    severity: medium
    situation: |
      A memory gets used in many successful decisions. Outcome adjustments
      accumulate until salience exceeds 1.0 or goes negative after failures.
    why: |
      Mind v5 outcome learning uses additive adjustment. Without bounds,
      salience loses meaning. A memory at salience 5.0 breaks ranking.
    solution: |
      class BoundedSalienceAdjuster:
          MIN_SALIENCE = 0.01
          MAX_SALIENCE = 1.0

          async def adjust_from_outcome(
              self,
              memory_id: UUID,
              adjustment: float,
          ) -> float:
              current = await self.db.get_salience(memory_id)

              # Apply bounded adjustment
              new_salience = current + adjustment
              new_salience = max(self.MIN_SALIENCE, min(self.MAX_SALIENCE, new_salience))

              # Log when hitting bounds
              if new_salience == self.MAX_SALIENCE or new_salience == self.MIN_SALIENCE:
                  logger.info(
                      "salience_hit_bound",
                      memory_id=str(memory_id),
                      attempted=current + adjustment,
                      bounded_to=new_salience,
                  )

              await self.db.update_salience(memory_id, new_salience)
              return new_salience
    symptoms:
      - "Salience values > 1.0 in database"
      - "Negative salience values"
      - "Top memories all have same max salience"
    detection_pattern: 'salience.*\\+.*adjustment(?!.*min|.*max|.*bound|.*clamp)'
    version_range: ">=5.0.0"
