# Performance Hunter Collaboration Model
# How this skill works with other AI memory specialists

prerequisites:
  skills: []
  knowledge:
    - "Understanding of async/await and concurrency"
    - "Basic database query understanding"
    - "Familiarity with profiling tools"
    - "Understanding of caching concepts"

complementary_skills:
  - skill: vector-specialist
    relationship: "Vector search optimization"
    brings: "HNSW tuning, quantization, index configuration"

  - skill: graph-engineer
    relationship: "Graph query optimization"
    brings: "Cypher optimization, index design, query patterns"

  - skill: temporal-craftsman
    relationship: "Workflow performance"
    brings: "Worker configuration, task queue design"

  - skill: event-architect
    relationship: "Event processing throughput"
    brings: "Consumer optimization, partition strategies"

  - skill: ml-memory
    relationship: "Memory retrieval latency"
    brings: "Memory access patterns, caching strategies"

  - skill: privacy-guardian
    relationship: "Encryption performance"
    brings: "Crypto overhead measurement, optimization"

  - skill: postgres-wizard
    relationship: "Database optimization"
    brings: "Query optimization, EXPLAIN ANALYZE, index tuning"

  - skill: observability-sre
    relationship: "Performance monitoring"
    brings: "SLO dashboards, latency percentile tracking, alerting"

  - skill: infra-architect
    relationship: "Infrastructure scaling"
    brings: "Horizontal scaling, auto-scaling, resource sizing"

  - skill: python-craftsman
    relationship: "Python performance"
    brings: "Async optimization, memory profiling, cProfile patterns"

delegation:
  - trigger: "vector search is slow"
    delegate_to: vector-specialist
    pattern: parallel
    context: "Current HNSW parameters and query patterns"
    receive: "Index tuning recommendations"

  - trigger: "graph queries are slow"
    delegate_to: graph-engineer
    pattern: parallel
    context: "Cypher queries and execution plans"
    receive: "Query optimization and index recommendations"

  - trigger: "workflow throughput is low"
    delegate_to: temporal-craftsman
    pattern: review
    context: "Worker configuration and task queue design"
    receive: "Worker tuning recommendations"

  - trigger: "event processing is slow"
    delegate_to: event-architect
    pattern: review
    context: "Consumer configuration and partition strategy"
    receive: "Consumer optimization recommendations"

  - trigger: "memory retrieval is slow"
    delegate_to: ml-memory
    pattern: parallel
    context: "Memory access patterns and retrieval pipeline"
    receive: "Memory system optimization recommendations"

collaboration_patterns:
  sequential:
    - "I profile system, then vector-specialist optimizes vector search"
    - "I identify bottleneck, then graph-engineer optimizes Cypher"

  parallel:
    - "I optimize caching while vector-specialist tunes HNSW"
    - "I optimize pooling while temporal-craftsman tunes workers"

  review:
    - "Review vector-specialist index configuration for performance"
    - "Review temporal-craftsman worker settings for throughput"
    - "Review privacy-guardian encryption for overhead"

cross_domain_insights:
  - domain: operating-systems
    insight: "Same principles: profiling, scheduling, caching, pooling"
    applies_when: "Thinking about resource management"

  - domain: queuing-theory
    insight: "Little's Law: L = λW (items = arrival rate × wait time)"
    applies_when: "Sizing connection pools and queues"

  - domain: computer-architecture
    insight: "Memory hierarchy: L1 cache (fast/small) → RAM (slow/big)"
    applies_when: "Designing multi-level cache"

  - domain: statistics
    insight: "Percentiles matter more than averages for user experience"
    applies_when: "Choosing latency metrics"

  - domain: economics
    insight: "Premature optimization has opportunity cost"
    applies_when: "Deciding when to optimize"

ecosystem:
  primary_tools:
    - "cProfile / py-spy - Python profiling"
    - "asyncio debug mode - Event loop analysis"
    - "Prometheus + Grafana - Metrics and alerting"
    - "locust / k6 - Load testing"

  alternatives:
    - name: Pyinstrument
      use_when: "Need easier-to-read profiler output"
      avoid_when: "Need precise timing or async profiling"

    - name: memory_profiler
      use_when: "Debugging memory leaks"
      avoid_when: "CPU profiling needed"

    - name: Jaeger/Zipkin
      use_when: "Distributed tracing across services"
      avoid_when: "Single service optimization"

    - name: pgBadger
      use_when: "PostgreSQL slow query analysis"
      avoid_when: "Not using PostgreSQL"

  deprecated:
    - "Guessing where bottleneck is"
    - "Optimizing without measurement"
    - "Average-only latency metrics"
    - "Sync I/O in async code"
    - "Connection per request pattern"

# Mind v5 Specific Context
mind_v5_specific:
  primary_responsibility: Performance optimization for Mind v5 retrieval and processing
  pod: quality
  critical_artifacts:
    - tests/benchmarks/**/*.py
    - src/core/memory/retrieval.py
    - src/infrastructure/**/*.py
  performance_targets:
    memory_retrieval:
      p50: 50ms
      p99: 200ms
      throughput: 1000 queries/sec
    event_processing:
      p50: 10ms
      p99: 100ms
      throughput: 400K events/sec
    causal_query:
      p50: 100ms
      p99: 500ms
    embedding_generation:
      p50: 20ms
      p99: 100ms
      batch_size: 100
  bottleneck_patterns:
    vector_search:
      symptoms: "High Qdrant latency in traces"
      investigation: "Check HNSW parameters, quantization"
      solution: "Tune ef_search, enable scalar quantization"
    graph_traversal:
      symptoms: "FalkorDB query timeout"
      investigation: "Check query complexity, missing indexes"
      solution: "Add composite indexes, limit traversal depth"
    event_consumer_lag:
      symptoms: "NATS consumer lag growing"
      investigation: "Check consumer throughput, batch size"
      solution: "Increase workers, optimize handler code"
  metrics_owned:
    - mind_retrieval_latency_histogram
    - mind_event_processing_latency_histogram
    - mind_cache_hit_ratio
    - mind_connection_pool_utilization
  delegation:
    - trigger: "vector search latency"
      delegate_to: vector-specialist
      pattern: parallel
      context: "Qdrant query patterns and latencies"
      receive: "HNSW tuning and quantization recommendations"

    - trigger: "graph query performance"
      delegate_to: graph-engineer
      pattern: parallel
      context: "FalkorDB queries and execution plans"
      receive: "Index recommendations and query optimization"

    - trigger: "database query performance"
      delegate_to: postgres-wizard
      pattern: parallel
      context: "PostgreSQL EXPLAIN ANALYZE output"
      receive: "Index and query optimization"

    - trigger: "Python code profiling"
      delegate_to: python-craftsman
      pattern: parallel
      context: "Profiling results and hot spots"
      receive: "Code optimization suggestions"
